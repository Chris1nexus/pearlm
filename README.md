# PERLM: Faithful Path-Based Explainable Recommendation via Language
This repository contains the source code of the submitted paper "PERLM: Faithful Path-Based Explainable Recommendation via Language
Modeling over Knowledge Graphs".

## Requirements
- Python 3.8

Install the required packages:
```pip install -r requirements.txt```

Download the datasets and the **embeddings**(to run the plm-rec implementation) from the **data.zip** and **embedding-weights.zip** archive at the drive repository: https://drive.google.com/drive/folders/1e0uFWb6iJ6MXHtslZsqV8qRYC0Pl_AR7?usp=sharing
Then extract both **data.zip** and **embedding-weights.zip** inside the **top level** of the repository (i.e. the level in which setup.py is located). 

## Usage
##### Design philosophy: 
The experiments which are reported in the paper can be run with ease by means of the provided bash scripts.
This holds both for dataset generation and model training.
To access the lower level details, one can directly use the python scripts which are called by the same bash scripts.

Note: all experiments have been run with fixed seed in order to ease reproducibility of the results.

### 0. Install the repository
From the top-level (i.e. the folder which contains setup.py and the pathlm folder)
Run:
```sh
pip install . 
```
### 1. Path Dataset generation
To generate all datasets, run from the top level:
```sh
source build_datasets.sh
```
Each dataset is generated by the pipeline described in 'create_dataset.sh' which is in charge of:
1. Generation of a dataset of **at most X unique paths per user**
2. Concatenation of the results into a single .txt file
3. (Optional) Pruning of the concatenated .txt file (This is only useful if the start entity is chosen instead of the standard 'USER')
4. Move of the concatenated and pruned .txt file into the 'data' folder which is used to tokenize and train the models

### 2. Training
From the top-level (i.e. the folder which contains setup.py and the pathlm folder).
Install the repository with ```pip install .```

Then, proceed according to the chosen experiment to run as described below.
Each bash script can be customised as desired in order to run alternative experiments
##### PERLM
To train PERLM, run from the top level:
```sh
source run_perlm_experiments.sh
```
##### PLM-Rec
To train PLM-Rec, run from the top level:
```sh
$CUDA_DEVICE_NUM=0
source run_plm-rec_experiments.sh $CUDA_DEVICE_NUM
```
