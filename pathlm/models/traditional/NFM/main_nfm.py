import logging
import os
import sys
import random
from time import time

import numpy as np
import pandas as pd
import torch

from pathlm.models.model_utils import EarlyStopping, save_model, load_model, logging_metrics
from pathlm.utils import SEED
from tqdm import tqdm
import scipy.sparse as sp
import torch.optim as optim
import torch.multiprocessing as mp

from pathlm.evaluation.eval_metrics import evaluate_rec_quality
from pathlm.evaluation.utility_metrics import RECALL, NDCG, PRECISION
from pathlm.models.traditional.NFM.nfm import NFM
from pathlm.models.traditional.NFM.dataloader_nfm import DataLoaderNFM
from pathlm.models.traditional.NFM.parser_nfm import parse_nfm_args
from pathlm.models.traditional.log_helper import create_log_id, logging_config
from pathlm.models.traditional.traditional_utils import early_stopping, compute_topks


def evaluate_batch(model, dataloader, user_ids, Ks):
    train_user_dict = dataloader.train_user_dict
    test_user_dict = dataloader.test_user_dict

    n_users = len(user_ids)
    n_items = dataloader.n_items
    item_ids = list(range(n_items))
    user_idx_map = dict(zip(user_ids, range(n_users)))

    feature_values = dataloader.generate_test_batch(user_ids)
    with torch.no_grad():
        scores = model(feature_values, is_train=False)              # (batch_size)

    rows = [user_idx_map[u] for u in np.repeat(user_ids, n_items).tolist()]
    cols = item_ids * n_users
    score_matrix = torch.Tensor(sp.coo_matrix((scores, (rows, cols)), shape=(n_users, n_items)).todense())

    user_ids = np.array(user_ids)
    item_ids = np.array(item_ids)
    metrics_dict = compute_topks(score_matrix, train_user_dict, test_user_dict, user_ids, item_ids, Ks)

    score_matrix = score_matrix.numpy()
    return score_matrix, metrics_dict


def evaluate(model, dataloader, Ks, device, num_processes=4):
    test_batch_size = dataloader.test_batch_size
    train_user_dict = dataloader.train_user_dict
    valid_user_dict = dataloader.valid_user_dict
    test_user_dict = dataloader.test_user_dict

    model.eval()

    user_ids = list(test_user_dict.keys())
    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]

    n_users = len(user_ids)
    n_items = dataloader.n_items
    item_ids = list(range(n_items))
    user_idx_map = dict(zip(user_ids, range(n_users)))

    cf_users = []
    cf_items = []
    cf_scores = []

    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:
        for batch_user in user_ids_batches:
            feature_values = dataloader.generate_test_batch(batch_user)
            feature_values = feature_values.to(device)

            with torch.no_grad():
                batch_scores = model(feature_values, is_train=False)            # (batch_size)

            cf_users.extend(np.repeat(batch_user, n_items).tolist())
            cf_items.extend(item_ids * len(batch_user))
            cf_scores.append(batch_scores.cpu())
            pbar.update(1)

    rows = [user_idx_map[u] for u in cf_users]
    cols = cf_items
    cf_scores = torch.cat(cf_scores)
    cf_score_matrix = torch.Tensor(sp.coo_matrix((cf_scores, (rows, cols)), shape=(n_users, n_items)).todense())

    user_ids = np.array(user_ids)
    item_ids = np.array(item_ids)
    topk_items_dict = compute_topks(cf_score_matrix, train_user_dict, valid_user_dict, test_user_dict, user_ids, item_ids, Ks)
    avg_metrics_dict = {k: evaluate_rec_quality(dataloader.dataset_name, topk_items_dict, test_user_dict, k)[1] for k in Ks}
    cf_score_matrix = cf_score_matrix.numpy()
    return cf_score_matrix, avg_metrics_dict


def train(args):
    # Set random seeds for reproducibility
    random.seed(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)

    # Setup logging
    log_save_id = create_log_id(args.log_dir)
    logging_config(folder=args.log_dir, name=f'log{log_save_id}', no_console=False)
    logging.info(args)

    # Setup device (GPU/CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load data
    data = DataLoaderNFM(args)

    # Initialize model
    user_pre_embed = torch.tensor(data.user_pre_embed) if args.use_pretrain == 1 else None
    item_pre_embed = torch.tensor(data.item_pre_embed) if args.use_pretrain == 1 else None
    model = NFM(args, data.n_users, data.n_items, data.n_entities, user_pre_embed, item_pre_embed).to(device)
    if args.use_pretrain == 2:
        model = load_model(model, args.pretrain_model_path)
    logging.info(model)

    # Setup optimizer
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # Initialize early stopping
    early_stopper = EarlyStopping(args.stopping_steps, verbose=True)
    best_ndcg_value = 0.0
    best_epoch = 0

    # Training loop
    for epoch in range(1, args.n_epoch + 1):
        model.train()

        # train cf
        time1 = time()
        total_loss = 0
        n_batch = data.n_cf_train // data.train_batch_size + 1

        for iter in range(1, n_batch + 1):
            time2 = time()
            pos_feature_values, neg_feature_values = data.generate_train_batch(data.train_user_dict)
            pos_feature_values = pos_feature_values.to(device)
            neg_feature_values = neg_feature_values.to(device)
            batch_loss = model(pos_feature_values, neg_feature_values, is_train=True)

            if np.isnan(batch_loss.cpu().detach().numpy()):
                #logging.info('ERROR: Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_batch))
                sys.exit(-1)

            batch_loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_loss += batch_loss.item()

            if (iter % args.print_every) == 0:
                logging.info(f'CF Training: Epoch {epoch:04d} Iter {iter:04d} / {n_batch:04d} | '
                             f'Time {time() - time2:.1f}s | Iter Loss {batch_loss.item():.4f} | '
                             f'Iter Mean Loss {total_loss / iter:.4f}')

        logging.info(f'CF Training: Epoch {epoch:04d} Total Iter {n_batch:04d} | Total Time {time() - time1:.1f}s | '
                     f'Iter Mean Loss {total_loss / n_batch:.4f}')

        if epoch % args.evaluate_every == 0 or epoch == args.n_epoch:
            _, metrics_dict = evaluate(model, data, args.Ks, device)
            is_best = metrics_dict[args.Ks[0]][NDCG] > best_ndcg_value
            best_ndcg_value = max(metrics_dict[args.Ks[0]][NDCG], best_ndcg_value)

            if is_best:
                save_model(model, args.weight_dir, args, epoch, best_epoch)
                best_epoch = epoch

            early_stopper(metrics_dict[args.Ks[0]][NDCG])
            if early_stopper.early_stop:
                logging.info('Early stopping triggered. Stopping training.')
                break

            logging_metrics(epoch, metrics_dict, args.Ks)

        if epoch % args.save_interval == 0:
            save_model(model, args.weight_dir_ckpt, args, epoch)

    # Final log for best metrics
    logging.info(f'Best evaluation results at epoch {best_epoch} with NDCG: {best_ndcg_value:.4f}')

def predict(args):
    # GPU / CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # load data
    data = DataLoaderNFM(args)

    # load model
    model = NFM(args, data.n_users, data.n_items, data.n_entities)
    model = load_model(model, args.pretrain_model_path)
    model.to(device)

    # predict
    num_processes = args.test_cores
    #if num_processes and num_processes > 1:
    #    evaluate_func = evaluate_mp
    #else:
    evaluate_func = evaluate

    Ks = eval(args.Ks)
    k_min = min(Ks)
    k_max = max(Ks)

    cf_scores, metrics_dict = evaluate_func(model, data, Ks, num_processes, device)
    np.save(args.save_dir + 'cf_scores.npy', cf_scores)
    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(
        metrics_dict[k_min][PRECISION], metrics_dict[k_max][PRECISION], metrics_dict[k_min][RECALL],
        metrics_dict[k_max][RECALL], metrics_dict[k_min][NDCG], metrics_dict[k_max][NDCG]))



if __name__ == '__main__':
    try:
        mp.set_start_method('spawn')
    except RuntimeError:
        pass

    args = parse_nfm_args()
    train(args)
    # predict(args)

